{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Movie Review Sentiment Analysis - Model Training and Evaluation\n",
    "\n",
    "This notebook demonstrates how to train and evaluate a BERT-based sentiment analysis model on the IMDB Movie Review dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.data.processor import get_data_processor\n",
    "from src.models.bert_classifier import get_model, save_model, load_model\n",
    "from src.utils.config import load_config\n",
    "from src.utils.metrics import compute_metrics, log_metrics, get_classification_report\n",
    "from src.utils.visualization import (\n",
    "    set_plotting_style,\n",
    "    save_figure,\n",
    "    plot_training_history,\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve,\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "set_plotting_style()\n",
    "\n",
    "# Create output directories\n",
    "models_dir = os.path.join(project_root, 'models')\n",
    "results_dir = os.path.join(project_root, 'models', 'results')\n",
    "visualizations_dir = os.path.join(project_root, 'models', 'visualizations')\n",
    "\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs(visualizations_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Let's load the configuration files for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "train_config_path = os.path.join(project_root, 'configs', 'train.yaml')\n",
    "data_config_path = os.path.join(project_root, 'configs', 'data_config.yaml')\n",
    "\n",
    "train_config = load_config(train_config_path)\n",
    "data_config = load_config(data_config_path)\n",
    "\n",
    "# Display configurations\n",
    "print(\"Training Configuration:\")\n",
    "print(yaml.dump(train_config, default_flow_style=False))\n",
    "\n",
    "print(\"\\nData Configuration:\")\n",
    "print(yaml.dump(data_config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's prepare the IMDB dataset for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "model_name = train_config['model']['name']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Initialized tokenizer: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data processor\n",
    "data_processor = get_data_processor(data_config, tokenizer, model_name)\n",
    "print(\"Initialized data processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset\n",
    "print(\"Loading dataset...\")\n",
    "try:\n",
    "    # Try to load from processed directory\n",
    "    train_dataset, val_dataset, test_dataset = data_processor.prepare_dataset(use_processed=True)\n",
    "    print(\"Loaded processed dataset\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading processed dataset: {e}\")\n",
    "    print(\"Loading dataset from Hugging Face...\")\n",
    "    \n",
    "    # Load from Hugging Face\n",
    "    imdb_dataset = load_dataset('imdb')\n",
    "    \n",
    "    # Convert to pandas DataFrame\n",
    "    train_df = pd.DataFrame(imdb_dataset['train'])\n",
    "    test_df = pd.DataFrame(imdb_dataset['test'])\n",
    "    \n",
    "    # Split train into train and validation\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    train_df, val_df = train_test_split(\n",
    "        train_df,\n",
    "        test_size=data_config['splitting']['val_size'],\n",
    "        random_state=data_config['splitting']['random_state'],\n",
    "        stratify=train_df['label'] if data_config['splitting']['stratify'] else None\n",
    "    )\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    def tokenize_dataset(df):\n",
    "        texts = df['text'].tolist()\n",
    "        labels = df['label'].tolist()\n",
    "        \n",
    "        encodings = tokenizer(\n",
    "            texts,\n",
    "            padding=data_config['preprocessing']['padding'],\n",
    "            truncation=data_config['preprocessing']['truncation'],\n",
    "            max_length=data_config['preprocessing']['max_seq_length'],\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(\n",
    "            encodings['input_ids'],\n",
    "            encodings['attention_mask'],\n",
    "            torch.tensor(labels)\n",
    "        )\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    train_dataset = tokenize_dataset(train_df)\n",
    "    val_dataset = tokenize_dataset(val_df)\n",
    "    test_dataset = tokenize_dataset(test_df)\n",
    "    \n",
    "    print(\"Dataset loaded and tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_dataloader, val_dataloader, test_dataloader = data_processor.create_dataloaders(\n",
    "    train_dataset, val_dataset, test_dataset\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_dataloader)}\")\n",
    "print(f\"Validation batches: {len(val_dataloader)}\")\n",
    "print(f\"Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "Let's initialize the BERT-based sentiment classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = get_model(\n",
    "    model_name=model_name,\n",
    "    num_labels=train_config['model']['num_labels'],\n",
    "    dropout_rate=train_config['model']['dropout_rate'],\n",
    "    gradient_checkpointing=train_config['model']['gradient_checkpointing'],\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Initialized model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Get optimizer parameters\n",
    "optimizer_config = train_config['optimizer']\n",
    "learning_rate = optimizer_config.get('learning_rate', 2e-5)\n",
    "weight_decay = optimizer_config.get('weight_decay', 0.01)\n",
    "adam_epsilon = optimizer_config.get('adam_epsilon', 1e-8)\n",
    "adam_betas = optimizer_config.get('adam_betas', (0.9, 0.999))\n",
    "\n",
    "# Prepare optimizer\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': weight_decay\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=learning_rate,\n",
    "    eps=adam_epsilon,\n",
    "    betas=adam_betas\n",
    ")\n",
    "\n",
    "# Calculate total number of training steps\n",
    "num_epochs = train_config['training']['num_epochs']\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "# Calculate number of warmup steps\n",
    "warmup_ratio = optimizer_config.get('warmup_ratio', 0.1)\n",
    "warmup_steps = int(total_steps * warmup_ratio)\n",
    "\n",
    "# Create scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: AdamW, Learning rate: {learning_rate}, Weight decay: {weight_decay}\")\n",
    "print(f\"Scheduler: Linear, Warmup steps: {warmup_steps}/{total_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Let's train the model on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device, fp16=False):\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize metrics\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    epoch_steps = 0\n",
    "    \n",
    "    # Initialize scaler for mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler() if fp16 else None\n",
    "    \n",
    "    # Training loop\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        if isinstance(batch, dict):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        else:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            batch = {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': labels\n",
    "            }\n",
    "        \n",
    "        # Forward pass with mixed precision if enabled\n",
    "        if fp16:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    token_type_ids=batch.get('token_type_ids'),\n",
    "                    labels=batch['labels']\n",
    "                )\n",
    "                loss = outputs['loss']\n",
    "        else:\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                token_type_ids=batch.get('token_type_ids'),\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "            loss = outputs['loss']\n",
    "        \n",
    "        # Backward pass with mixed precision if enabled\n",
    "        if fp16:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Update metrics\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        logits = outputs['logits']\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        accuracy = (predictions == batch['labels']).float().mean().item()\n",
    "        epoch_accuracy += accuracy\n",
    "        \n",
    "        # Update progress bar\n",
    "        epoch_steps += 1\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': epoch_loss / epoch_steps,\n",
    "            'accuracy': epoch_accuracy / epoch_steps\n",
    "        })\n",
    "    \n",
    "    # Compute epoch metrics\n",
    "    epoch_metrics = {\n",
    "        'loss': epoch_loss / epoch_steps,\n",
    "        'accuracy': epoch_accuracy / epoch_steps\n",
    "    }\n",
    "    \n",
    "    return epoch_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device, fp16=False):\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize metrics\n",
    "    eval_loss = 0.0\n",
    "    eval_steps = 0\n",
    "    \n",
    "    # Initialize lists for predictions and labels\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    \n",
    "    # Evaluation loop\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Evaluating\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # Move batch to device\n",
    "            if isinstance(batch, dict):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            else:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "                batch = {\n",
    "                    'input_ids': input_ids,\n",
    "                    'attention_mask': attention_mask,\n",
    "                    'labels': labels\n",
    "                }\n",
    "            \n",
    "            # Forward pass with mixed precision if enabled\n",
    "            if fp16:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(\n",
    "                        input_ids=batch['input_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        token_type_ids=batch.get('token_type_ids'),\n",
    "                        labels=batch['labels']\n",
    "                    )\n",
    "                    loss = outputs['loss']\n",
    "            else:\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    token_type_ids=batch.get('token_type_ids'),\n",
    "                    labels=batch['labels']\n",
    "                )\n",
    "                loss = outputs['loss']\n",
    "            \n",
    "            # Update metrics\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            # Get predictions and labels\n",
    "            logits = outputs['logits']\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            # Append to lists\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_labels.append(batch['labels'].cpu().numpy())\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            eval_steps += 1\n",
    "            progress_bar.set_postfix({'loss': eval_loss / eval_steps})\n",
    "    \n",
    "    # Concatenate predictions and labels\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_logits = np.concatenate(all_logits)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics(all_predictions, all_labels)\n",
    "    metrics['loss'] = eval_loss / eval_steps\n",
    "    \n",
    "    return metrics, all_predictions, all_labels, all_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# Initialize variables for early stopping\n",
    "best_val_metric = float(\"-inf\")\n",
    "best_epoch = 0\n",
    "patience = train_config['training']['early_stopping_patience']\n",
    "patience_counter = 0\n",
    "\n",
    "# Initialize training history\n",
    "history = {\n",
    "    'loss': [],\n",
    "    'accuracy': [],\n",
    "    'val_loss': [],\n",
    "    'val_accuracy': [],\n",
    "    'val_f1': []\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_metrics = train_epoch(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        fp16=train_config['training']['fp16']\n",
    "    )\n",
    "    \n",
    "    # Log training metrics\n",
    "    print(f\"Training metrics:\")\n",
    "    for name, value in train_metrics.items():\n",
    "        print(f\"  {name}: {value:.4f}\")\n",
    "    \n",
    "    # Update history\n",
    "    history['loss'].append(train_metrics['loss'])\n",
    "    history['accuracy'].append(train_metrics['accuracy'])\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    print(\"\\nEvaluating on validation set...\")\n",
    "    val_metrics, val_predictions, val_labels, val_logits = evaluate(\n",
    "        model,\n",
    "        val_dataloader,\n",
    "        device,\n",
    "        fp16=train_config['training']['fp16']\n",
    "    )\n",
    "    \n",
    "    # Log validation metrics\n",
    "    print(f\"Validation metrics:\")\n",
    "    for name, value in val_metrics.items():\n",
    "        print(f\"  {name}: {value:.4f}\")\n",
    "    \n",
    "    # Update history\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "    history['val_f1'].append(val_metrics['f1'])\n",
    "    \n",
    "    # Check if this is the best model so far\n",
    "    monitor_metric = val_metrics[train_config['logging']['monitor']]\n",
    "    \n",
    "    if (\n",
    "        train_config['logging']['mode'] == \"max\" and monitor_metric > best_val_metric\n",
    "    ) or (\n",
    "        train_config['logging']['mode'] == \"min\" and monitor_metric < best_val_metric\n",
    "    ):\n",
    "        print(\n",
    "            f\"New best model! {train_config['logging']['monitor']}: \"\n",
    "            f\"{monitor_metric:.4f} (previous: {best_val_metric:.4f})\"\n",
    "        )\n",
    "        \n",
    "        # Update best metric and epoch\n",
    "        best_val_metric = monitor_metric\n",
    "        best_epoch = epoch\n",
    "        \n",
    "        # Reset patience counter\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save model\n",
    "        print(f\"Saving model to {models_dir}\")\n",
    "        save_model(model, tokenizer, models_dir, \"best_model\")\n",
    "        \n",
    "        # Save training history\n",
    "        with open(os.path.join(models_dir, \"history.json\"), \"w\") as f:\n",
    "            json.dump(history, f)\n",
    "    else:\n",
    "        # Increment patience counter\n",
    "        patience_counter += 1\n",
    "        \n",
    "        print(\n",
    "            f\"No improvement over best model. \"\n",
    "            f\"Patience: {patience_counter}/{patience}\"\n",
    "        )\n",
    "        \n",
    "        # Check if we should stop training\n",
    "        if patience_counter >= patience:\n",
    "            print(\n",
    "                f\"Early stopping triggered after {epoch + 1} epochs. \"\n",
    "                f\"Best {train_config['logging']['monitor']}: {best_val_metric:.4f} \"\n",
    "                f\"at epoch {best_epoch + 1}\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training History\n",
    "\n",
    "Let's visualize the training history to see how the model performed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plot_training_history(history, visualizations_dir, filename=\"training_history\")\n",
    "\n",
    "# Display the plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Plot training and validation loss\n",
    "axes[0].plot(history['loss'], label='Training Loss')\n",
    "axes[0].plot(history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "axes[1].plot(history['accuracy'], label='Training Accuracy')\n",
    "axes[1].plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set\n",
    "\n",
    "Let's evaluate the best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model_path = os.path.join(models_dir, \"best_model\")\n",
    "model, tokenizer = load_model(best_model_path, device)\n",
    "\n",
    "print(f\"Loaded best model from {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_metrics, test_predictions, test_labels, test_logits = evaluate(\n",
    "    model,\n",
    "    test_dataloader,\n",
    "    device,\n",
    "    fp16=train_config['training']['fp16']\n",
    ")\n",
    "\n",
    "# Log test metrics\n",
    "print(f\"Test metrics:\")\n",
    "for name, value in test_metrics.items():\n",
    "    print(f\"  {name}: {value:.4f}\")\n",
    "\n",
    "# Save test metrics\n",
    "test_metrics_path = os.path.join(results_dir, \"test_metrics.json\")\n",
    "with open(test_metrics_path, \"w\") as f:\n",
    "    json.dump(test_metrics, f, indent=4)\n",
    "\n",
    "# Get classification report\n",
    "class_names = ['Negative', 'Positive']\n",
    "report = get_classification_report(test_predictions, test_labels, class_names)\n",
    "print(f\"\\nClassification report:\\n{report}\")\n",
    "\n",
    "# Save classification report\n",
    "report_path = os.path.join(results_dir, \"test_classification_report.txt\")\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Test Set Results\n",
    "\n",
    "Let's visualize the results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    test_labels,\n",
    "    test_predictions,\n",
    "    class_names,\n",
    "    visualizations_dir,\n",
    "    filename=\"test_confusion_matrix\"\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "cm_fig, cm_ax = plt.subplots(figsize=(8, 6))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=cm_ax)\n",
    "cm_ax.set_xlabel('Predicted Label')\n",
    "cm_ax.set_ylabel('True Label')\n",
    "cm_ax.set_title('Confusion Matrix - Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "if len(class_names) == 2 and test_logits.shape[1] == 2:\n",
    "    positive_probs = torch.softmax(torch.tensor(test_logits), dim=1)[:, 1].numpy()\n",
    "    plot_roc_curve(\n",
    "        test_labels,\n",
    "        positive_probs,\n",
    "        visualizations_dir,\n",
    "        filename=\"test_roc_curve\"\n",
    "    )\n",
    "    \n",
    "    # Display the plot\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, _ = roc_curve(test_labels, positive_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    roc_fig, roc_ax = plt.subplots(figsize=(8, 6))\n",
    "    roc_ax.plot(fpr, tpr, lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    roc_ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    roc_ax.set_xlim([0.0, 1.0])\n",
    "    roc_ax.set_ylim([0.0, 1.05])\n",
    "    roc_ax.set_xlabel('False Positive Rate')\n",
    "    roc_ax.set_ylabel('True Positive Rate')\n",
    "    roc_ax.set_title('Receiver Operating Characteristic (ROC) Curve - Test Set')\n",
    "    roc_ax.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've trained and evaluated a BERT-based sentiment analysis model on the IMDB dataset. The model's performance can be further improved by hyperparameter tuning, using a larger BERT model, or exploring other techniques like data augmentation.\n",
    "\n",
    "The trained model and evaluation results are saved in the `models/` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
